{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week14_Federated_Learning_210524.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNo1hLNK3IdC/dgg7weBSjT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"bV8CKWKRtAQE"},"source":["import os\n","import random\n","import time\n","import copy\n","import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import matplotlib.pyplot as plt\n","\n","from multiprocessing import cpu_count\n","from datetime import timedelta\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOOysHXtuEan"},"source":["## Fix Seed"]},{"cell_type":"code","metadata":{"id":"8jjZDvSTtNmb"},"source":["SEED = 42\n","\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.benchmark = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDvgMy3fuIdK"},"source":["## Assign Device"]},{"cell_type":"code","metadata":{"id":"BVhV1nsquMA8"},"source":["GPU = 0\n","\n","device = torch.device(f'cuda:{GPU}' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvEal-HkuMnB"},"source":["## Augment Data"]},{"cell_type":"code","metadata":{"id":"ixoPtzF0uOHa"},"source":["transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n","])\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])    \n","])\n","\n","train_dataset = CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n","test_dataset = CIFAR10(root='./data', train=False, transform=transform_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRQpzFfvvtvr"},"source":["## Define Model"]},{"cell_type":"code","metadata":{"id":"6jkq6k0vvvJ3"},"source":["class BottleNeck(nn.Module):\n","    def __init__(self, in_planes, growth_rate):\n","        super().__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n","        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.conv1(F.relu(self.bn1(x)))\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = torch.cat([out, x], 1)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRdfOv6ORteF"},"source":["class Transition(nn.Module):\n","    def __init__(self, in_planes, out_planes):\n","        super().__init__()\n","        self.bn = nn.BatchNorm2d(in_planes)\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.conv(F.relu(self.bn(x)))\n","        out = F.avg_pool2d(out, 2)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9_u0J5x-wTNm"},"source":["class DenseNet(nn.Module):\n","    def __init__(self, growth_rate=12, reduction=0.5, num_classes=10):\n","        super().__init__()\n","        self.growth_rate = growth_rate\n","\n","        num_planes = 2 * growth_rate\n","        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n","\n","        self.dense1 = self._make_dense_layers(BottleNeck, num_planes, 6)\n","        num_planes += 6 * growth_rate\n","        out_planes = int(math.floor(num_planes * reduction))\n","        self.trans1 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense2 = self._make_dense_layers(BottleNeck, num_planes, 12)\n","        num_planes += 12 * growth_rate\n","        out_planes = int(math.floor(num_planes * reduction))\n","        self.trans2 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense3 = self._make_dense_layers(BottleNeck, num_planes, 24)\n","        num_planes += 24 * growth_rate\n","        out_planes = int(math.floor(num_planes * reduction))\n","        self.trans3 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense4 = self._make_dense_layers(BottleNeck, num_planes, 16)\n","        num_planes += 16 * growth_rate\n","\n","        self.bn = nn.BatchNorm2d(num_planes)\n","        self.linear = nn.Linear(num_planes, num_classes)\n","\n","    def _make_dense_layers(self, block, in_planes, nblock):\n","        layers = []\n","        for i in range(nblock):\n","            layers.append(block(in_planes, self.growth_rate))\n","            in_planes += self.growth_rate\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.trans1(self.dense1(out))\n","        out = self.trans2(self.dense2(out))\n","        out = self.trans3(self.dense3(out))\n","        out = self.dense4(out)\n","        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UQ7jf6evoYq"},"source":["## Split Data"]},{"cell_type":"code","metadata":{"id":"1i6uBwbRtnTO"},"source":["BATCH_SIZE = 2 ** 8\n","\n","if cpu_count() > 5:\n","    NUM_WORKERS = cpu_count() // 2\n","elif cpu_count() < 2:\n","    NUM_WORKERS = 0\n","else:\n","    NUM_WORKERS = 2\n","\n","testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgCo5gO0tTIr"},"source":["num_models = 10\n","\n","x_idxs = np.arange(len(train_dataset))\n","ys = train_dataset.targets.copy()\n","splits = []\n","for i in range(num_models):\n","    x_idxs, x_splits, ys, _  = train_test_split(x_idxs, ys, test_size=1/num_models, random_state=SEED, shuffle=True, stratify=ys)\n","    splits.append(x_splits)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWZZHap2QhzO"},"source":["trainloaders = []\n","\n","for idxs in splits:\n","    sampler = SubsetRandomSampler(idxs)\n","    trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS)\n","    trainloaders.append(trainloader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ru7wrZKgQjyW"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"nrmBbVs1BG6t"},"source":["fed_model = DenseNet().to(device)\n","fed_weights = fed_model.state_dict()\n","\n","criterion = nn.CrossEntropyLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nOKXwv-wOm-7"},"source":["def average_weights(w):\n","    w_avg = copy.deepcopy(w[0])\n","    for key in w_avg.keys():\n","        for i in range(1, len(w)):\n","            w_avg[key] += w[i][key]\n","        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n","    return w_avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UW1tkNZ1-R-e"},"source":["def train(model, loader, epochs, lr, momentum, weight_decay, criterion, device):\n","    losses = []\n","    \n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","    \n","    model.train()\n","\n","    for ep in range(epochs):\n","        batch_ls = []\n","        \n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            model.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_ls.append(loss.item())\n","\n","        loss_avg = sum(batch_ls) / len(batch_ls)\n","        losses.append(loss_avg)\n","    return model.state_dict(), losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVrX_kLXB96I"},"source":["def test(model, testloader, criterion, device):\n","    loss, correct, total = 0, 0, 0\n","\n","    model.eval()\n","\n","    for images, labels in testloader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        batch_ls = criterion(outputs, labels)\n","        loss += batch_ls.item()\n","\n","        _, preds = torch.max(outputs, 1)\n","        preds = preds.view(-1)\n","        correct += torch.sum(torch.eq(preds, labels)).item()\n","        total += len(labels)\n","\n","    accuracy = correct / total\n","    loss /= len(testloader)\n","    return accuracy, loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDU0GRdbOoka"},"source":["ROUNDS = 5\n","EPOCHS = 5\n","LR = 0.01\n","MOMENTUM = 0.9\n","WEIGHT_DECAY = 0.0005\n","\n","train_losses, test_accs, test_losses = [], [], []\n","\n","st = time.time()\n","for r in range(ROUNDS):\n","    local_weights, local_losses = [], []\n","    print(f'\\n | Global Training Round : {r + 1} / {ROUNDS} |')\n","    \n","    fed_model.train()\n","    \n","    for tr in trainloaders:\n","        w, ls = train(copy.deepcopy(fed_model), tr, EPOCHS, LR, MOMENTUM, WEIGHT_DECAY, criterion, device)\n","        local_weights.append(copy.deepcopy(w))\n","        train_losses.append(ls)\n","        print('  |-- [Client {:>2}] Average Train Loss: {:.4f} ... {} local epochs'.format(i + 1, sum(ls) / len(ls), EPOCHS))\n","        \n","    fed_weights = average_weights(local_weights)\n","    fed_model.load_state_dict(fed_weights)\n","    \n","    test_acc, test_ls = test(fed_model, testloader, criterion, device)\n","    test_accs.append(test_acc)\n","    test_losses.append(test_ls)\n","    print('    |---- Test Accuracy: {:.4f}%'.format(100 * test_acc))\n","    print('    |---- Test Loss: {:.4f}'.format(test_ls))\n","    print('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKCnVoFtOy-z"},"source":["train_losses = np.asarray(train_losses)\n","\n","fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 9))\n","axs = axs.ravel()\n","\n","axs[0].plot(test_accs, c='orange')\n","axs[0].set_title('Test Accuracies')\n","axs[0].set_xlabel('Rounds')\n","axs[1].set_ylabel('Test Accuracy')\n","axs[1].plot(test_losses, c='purple')\n","axs[1].set_title('Test Losses')\n","axs[1].set_xlabel('Rounds')\n","axs[1].set_ylabel('Test Loss')\n","axs[2].plot(train_losses.mean(axis=1), c='red')\n","axs[2].set_title('Train Average Losses')\n","axs[2].set_xlabel('Epochs')\n","axs[2].set_ylabel('Train Average Loss')\n","axs[3].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='blue')\n","axs[3].set_title('Train Average Losses')\n","axs[3].set_xlabel('Rounds')\n","axs[3].set_ylabel('Train Average Loss')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlPR7tiAWku3"},"source":[""],"execution_count":null,"outputs":[]}]}